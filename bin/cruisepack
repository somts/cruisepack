#!/usr/bin/env python3

# TODO: deal with errant symlinks; they cause an exception in os.path.getsize
# TODO: erase source dir when user requests it

'''cruisepack
    STS cruise data packer, in Python'''

import argparse
import bz2
import gzip
import hashlib
import logging
import logging.handlers
import lzma
import os
import tarfile
import tempfile
import time
import sys
from datetime import timedelta
from json import dumps
from multiprocessing import cpu_count
from multiprocessing import Pool
from multiprocessing.dummy import Pool as DummyPool
from pathlib import Path
from shutil import copyfileobj
from zipfile import ZipFile, ZIP_BZIP2, ZIP_LZMA


class CPError(Exception):
    '''Hack our own fatal error class'''
    def __init__(self, errmsg, exception=None):
        super().__init__()
        self.errmsg = errmsg
        self.exception = exception

    def __str__(self):
        if self.exception is not None:
            return '\n\t'.join((self.errmsg, '%s' % self.exception))
        return self.errmsg


class CruisePack:
    '''Set up a class to pack cruise data'''
    TIME_DURATION_UNITS = (('week', 604800), ('day', 86400),
                           ('hour', 3600), ('min', 60), ('second', 1))

    def __init__(self):
        atime = time.perf_counter()
        self.args = self.args_validate(self.args_parse())

        # Set up an empty list for self.args.compress_after == True
        self.compress_after = []

        # Set up console logging
        self.formatter = logging.Formatter(
            '%(asctime)s %(levelname)7s: %(message)s')
        self.logger = self.logger_config()

        self.logger.info('CLI arguments:\n%s',
                         dumps(self.args.__dict__, indent=1))

        for i, srcdir in enumerate(self.args.source_dir):
            self.pack_dir(srcdir, self.args.dstlog[i],
                          self.args.dstmd5[i], self.args.dstzip[i])

        # Multithread compression at the end, when requested
        if len(self.compress_after) > 0:
            self.logger.info('Compressing delayed tarballs')
            with DummyPool(processes=self.args.proc_count) as pool:
                pool.starmap(self.compress, self.compress_after)

        self.logger.info('Completed all processing. %s', self.timesince(atime))

    @staticmethod
    def args_parse():
        '''parse STDIN, if any'''

        parser = argparse.ArgumentParser(
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            description='''
                Build and execute a series of CLI rsync commands
                in order to synchronize data from our
                Data Acquisition Systems (DAS) to a central server.'''
        )

        parser.add_argument(
            '-s', '--source-dir',
            nargs='+',
            required=True,
            help='source directory(s) of files to archive. Multiple sources' +
            ' will create multiple destinations')
        parser.add_argument(
            '-d', '--destination-dir',
            default=None,
            nargs='+',
            help='destination directiory(s) to write arcive to. When None, ' +
            'the destination will be the parent dir of each source')
        parser.add_argument(
            '-C', '--compress-after',
            action='store_true',
            help='Compress tarball(s) after successfully writing to all ' +
            'destination(s). Given ample disk space, this can be useful ' +
            'for batch copying multiple cruise datasets to the archival ' +
            'space as quickly as possible, then running compression, which ' +
            'takes the longest amount of time. Futher, since compression is ' +
            'a singleton process, mutlithreading a number of jobs at the ' +
            'end can mean less overall wait time, when important')
        parser.add_argument(
            '-D', '--delete-after',
            action='store_true',
            help='Delete source(s) after successfully writing destination(s).')
        parser.add_argument(
            '-E', '--extension',
            type=str,
            default='tar.bz2',
            choices=('tar.bz2', 'tar.gz', 'tar.xz', 'tar', 'zip'),
            help='extension of target archive. NOTE: zip processing is ' +
            'in series whereas tar.bz2 is multithreaded')
        parser.add_argument(
            '-O', '--overwrite-destination',
            action='store_true',
            help='Overwrite destination(s), if destination(s) exist.')
        parser.add_argument(
            '-P', '--proc-count',
            default=cpu_count()*2-1,
            type=int,
            help='number of parallel md5sum jobs to run')

        return parser.parse_args()

    @staticmethod
    def args_validate(args):
        '''Validate CLI arguments'''
        if not isinstance(args.delete_after, bool):
            raise CPError

        if not isinstance(args.source_dir, list):
            raise CPError
        # Convert args to absolute path(s)
        for i, val in enumerate(args.source_dir):
            args.source_dir[i] = Path(val).absolute().__str__()

        if args.destination_dir is None:
            args.destination_dir = []
            for i in args.source_dir:
                args.destination_dir.append(
                    Path(i).parent.absolute().__str__())

        if not isinstance(args.destination_dir, list):
            raise CPError

        # Calculate destinations
        dst = []

        if len(args.destination_dir) == 1:
            for i in args.source_dir:
                dst.append(
                    Path.joinpath(Path(args.destination_dir[0]),
                                  '%s' % Path(i).name).__str__())
        elif len(args.source_dir) == len(args.destination_dir):
            for i, val in enumerate(args.destination_dir):
                dst.append(
                    Path.joinpath(Path(val),
                                  '%s' % Path(
                                      args.source_dir[i]).name).__str__())
        else:
            raise CPError(
                "Don't know how to process %d sources and %d destinations"
                % (len(args.source_dir), len(args.destination_dir)))

        args.dstlog = ['%s.log.txt' % i for i in dst]
        args.dstmd5 = ['%s.md5.txt' % i for i in dst]
        args.dstzip = ['%s.%s' % (i, args.extension) for i in dst]

        return args

    def chkdst(self, dstfiles):
        '''Check to see if destination files exist and decide what to
           about it'''
        process = False
        if isinstance(dstfiles, str):  # ensure list
            dstfiles = [dstfiles]
        if not isinstance(dstfiles, list):  # ensure list
            raise CPError('chkdst requires a str on list of str')
        for i in dstfiles:
            if not os.path.exists(i):  # good to go
                process = True
            elif self.args.overwrite_destination is True:  # also good
                # Avoid overwriting file(s) if we did not call for it
                self.logger.warning(
                    '%s exists. overwrite_destination enabled. Overwriting!',
                    i)
                process = True
            else:
                # Avoid overwriting file(s) if we did not call for it
                self.logger.error(
                    '%s exists and overwrite_destination not set.',
                    i)
                process = False
        if process is False:
            self.logger.warning('Exiting due to exisiting files.')
            sys.exit(1)

    def compress(self, src, dst, compress):
        '''Compress a file'''
        # Validation
        self.chkdst(dst)
        if compress not in ('bz2', 'gz', 'xz'):
            raise CPError('Unsupported compression type, %s' % compress)
        srcsize = os.path.getsize(src)

        # Create compressed file
        atime = time.perf_counter()
        self.logger.info('Compress %s to %s (this can take a while...)',
                         src, dst)

        try:
            with open(src, 'rb') as fpsrc:
                if compress == 'bz2':
                    with bz2.open(dst, 'wb') as fpdst:
                        copyfileobj(fpsrc, fpdst)
                elif compress == 'gz':
                    with gzip.open(dst, 'wb') as fpdst:
                        copyfileobj(fpsrc, fpdst)
                elif compress == 'xz':
                    with lzma.open(dst, 'wb') as fpdst:
                        copyfileobj(fpsrc, fpdst)
        except KeyboardInterrupt:
            self.logger.warning(
                'Received keyboard interrupt. Deleting %s and quitting',
                dst)
            os.unlink(dst)
            sys.exit(3)

        self.logger.info('Done creating %s. %s', dst, self.timesince(atime))
        dstsize = os.path.getsize(dst)
        self.logger.info(
            'Disk usage of %s is %s (%0.2f%% smaller than %s)', dst,
            self.humanbytes(dstsize), 100-100*(dstsize/srcsize), src)
        self.logger.debug('Attempt to delete %s.', src)
        os.unlink(src)
        self.logger.debug('Deleted %s.', src)

    def getfilelist(self, path):
        '''Get a list of all files under a directory'''
        filelist = []
        self.logger.debug('Obtain list of files under %s', path)
        atime = time.perf_counter()

        for root, _, files in os.walk(path):
            for myfile in files:
                fullpath = os.path.join(root, myfile)
                filelist.append(fullpath)
        self.logger.info('Found %d files in %s. %s',
                         len(filelist), path, self.timesince(atime))
        return filelist

    @staticmethod
    def humanbytes(num):
        '''Convert a number (bytes) to human-readable bytes'''
        symbols = ('K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')
        prefix = {}
        for i, sym in enumerate(symbols):
            prefix[sym] = 1 << (i+1)*10
        for sym in reversed(symbols):
            if num >= prefix[sym]:
                return '%.1f%sB' % (num/prefix[sym], sym)
        return '%sB' % num

    def logger_config(self):
        '''Set up class logger'''
        self.logconf = argparse.Namespace(
            consolehdlr=logging.StreamHandler(), filehdlr=None)
        logger = logging.getLogger('CruisePack')

        # pass all messages down to handlers
        logger.setLevel(logging.DEBUG)

        # console logger
        self.logconf.consolehdlr.setLevel(logging.INFO)
        self.logconf.consolehdlr.setFormatter(self.formatter)
        logger.addHandler(self.logconf.consolehdlr)

        return logger

    def logger_filehandler_add(self, dstlog):
        '''Manage logging.FileHandler. We only ever have one open at
           a time'''

        try:
            self.logconf.filehdlr = logging.FileHandler(dstlog, mode='w')
        except (FileNotFoundError, PermissionError) as err:
            raise CPError('Not able to open log file %s! %s' % (dstlog, err))

        # Add new FileHandler to logger
        self.logconf.filehdlr.setFormatter(self.formatter)
        self.logconf.filehdlr.setLevel(logging.DEBUG)
        self.logger.addHandler(self.logconf.filehdlr)

    @staticmethod
    def md5sum(path):
        '''Open a file, calculate md5sum'''
        hash_md5 = hashlib.md5()
        try:
            with open(path, 'rb') as filep:
                for chunk in iter(lambda: filep.read(4096), b""):
                    hash_md5.update(chunk)
        except FileNotFoundError as err:
            # Sometimes datasets have symlinks from other filesystems
            # that will not resolve, so we'll just note that
            if os.path.islink(path):
                return (path, 'Nonsense symlink')
            else:
                raise CPError(err)
        return (path, hash_md5.hexdigest())

    def mkarchive(self, filename, files, md5file):
        '''Make a supported archive file'''
        self.logger.debug('"%s" mode selected', self.args.extension)

        # Rename compressed tarball filename to just .tar
        compression = False
        if filename.endswith(('.tar.bz2', '.tar.gz', '.tar.xz')):
            strings = filename.split('.')
            filename_orig = filename
            filename = '.'.join(strings[:-1])
            compression = strings[-1]
            self.logger.info(
                'Compression "%s" will be processed after tarball creation.',
                compression)

        # Calculate a unique filename in our destination dir
        _, tmpf = tempfile.mkstemp(dir=Path(filename).parent,
                                   prefix='%s.' % Path(filename).name)

        self.logger.info('Write %s (will later move to %s).', tmpf, filename)

        try:
            if self.args.extension == 'zip':
                # stackoverflow.com/questions/9195206/is-python-zipfile-thread-safe
                # zipfile is supposedly thread safe, but not in testing.
                # Limit threadcount to 1 to avoid concurrency errors.
                self.mkzip(tmpf, files, md5file, 1)
            else:
                # Compression is based on extension selected
                self.mktar(tmpf, files, md5file, self.args.proc_count)
        except KeyboardInterrupt:
            self.logger.warning(
                'Received keyboard interrupt. Deleting %s and quitting',
                tmpf)
            os.unlink(tmpf)
            sys.exit(2)

        self.logger.info('Wrote %s.', tmpf)

        self.move(tmpf, filename)

        self.logger.info('%s size is %s; mdsum is %s',
                         filename,
                         self.humanbytes(os.path.getsize(filename)),
                         self.md5sum(filename)[1])

        # Conditionally compress file (now or later) and return filename
        # of where we are leaving things.
        if compression:
            if self.args.compress_after:
                self.compress_after.append((filename, filename_orig,
                                            compression))
                self.logger.warning('Creating %s->%s has been delayed',
                                    filename, filename_orig)
                return filename
            else:
                self.compress(filename, filename_orig, compression)
                return filename_orig
        return filename

    @staticmethod
    def mktar(filename, files, md5file, threadcount):
        '''Make a tarball based provided filename and list of files'''

        # Only write an uncompressed .tar file; we want to copy raw
        # bytes to our archive location as quickly as possible, and
        # then save disk space, afterwards.
        with tarfile.open(filename, 'w:') as archive:
            archive.add(md5file)
            # Multithread add files to our archive. This speeds things
            # up, but not dramatically.
            with DummyPool(processes=threadcount) as pool:
                pool.map(archive.add, files)

    @staticmethod
    def mkzip(filename, files, md5file, threadcount):
        '''Make a zip file based provided filename and list of files'''
        try:
            # Failover to LZMA if needed
            archive = ZipFile(filename, 'w', compression=ZIP_BZIP2)
        except NotImplementedError:
            # If we don't have LZMA nor BZIP2 capability, fail
            archive = ZipFile(filename, 'w', compression=ZIP_LZMA)
        else:
            archive.write(md5file)
            # Multithread add files to our archive. This speeds things
            # up, but not dramatically.
            with DummyPool(processes=threadcount) as pool:
                pool.map(archive.write, files)
            archive.close()

    def move(self, src, dst):
        '''Move a file'''
        self.chkdst(dst)
        try:
            self.logger.info('Move %s to %s.', src, dst)
            os.rename(src, dst)
        except FileExistsError:
            if self.args.overwrite_destination:
                self.logger.warning('%s exists. Removing.', dst)
                os.remove(dst)
                os.rename(src, dst)
                return
            self.logger.error(
                '%s already exists. Quit and clean up %s.',
                dst, src)
            os.remove(src)
            sys.exit(3)
        except PermissionError as err:
            raise CPError('Cannot move %s to %s. %s' % (src, dst, err))

    def pack_dir(self, srcdir, dstlog, dstmd5, dstzip):
        '''Dig through a dir, md5sum every file and archive the dir'''
        atime = time.perf_counter()
        disclaimer = '(this can take a while)...'
        srcpath = Path(srcdir)

        self.chkdst([dstzip, dstlog, dstmd5])

        # Add relevant FileHandler to logging
        self.logger_filehandler_add(dstlog)
        self.logger.info('Processing directory %s', srcdir)
        self.logger.info('Start logging to %s', dstlog)

        cwd = os.getcwd()

        try:
            self.logger.info('Change directory from %s to %s',
                             cwd, srcpath.parent.__str__())
            os.chdir(srcpath.parent.__str__())
        except (FileNotFoundError, PermissionError) as err:
            self.logger.error(
                'Cannot `cd` to %s. Skip processing %s. %s',
                srcpath.parent.__str__(), srcdir, err)
            return
        else:
            filelist = self.getfilelist(srcpath.name)

            # Multiprocess file metadata operations
            with Pool(processes=self.args.proc_count) as pool:
                # Calculate sum of file sizes for every file on our list
                btime = time.perf_counter()
                self.logger.debug(
                    'Calculate total disk size used by %d files in %s %s...',
                    len(filelist), srcpath.name, disclaimer)
                bytes_used = sum(pool.map(os.path.getsize, filelist))
                self.logger.info(
                    'Total disk usage of %s: %s. %s',
                    srcpath.name, self.humanbytes(bytes_used),
                    self.timesince(btime))

                # Calculate md5sum for every file on our list
                ctime = time.perf_counter()
                self.logger.debug(
                    'Calculate md5sum on %d files in %s %s...',
                    len(filelist), srcpath.name, disclaimer)
                md5s = pool.map(self.md5sum, filelist)
                md5s.sort()

                # Save sorted list to a plaintext file
                self.logger.info(
                    'md5sum results saved to %s. %s',
                    dstmd5, self.timesince(ctime))
                with open(dstmd5, 'w') as filep:
                    filep.write('\n'.join(
                        ['{}\t{}'.format(i[1], i[0]) for i in md5s]))

            dtime = time.perf_counter()
            self.logger.info('Create %s %s', dstzip, disclaimer)
            filename = self.mkarchive(dstzip, filelist, dstmd5)
            self.logger.info('Created %s. %s', filename,
                             self.timesince(dtime))

        self.logger.info('Change directory from %s to %s',
                         srcpath.parent.__str__(), cwd)
        os.chdir(cwd)

        self.logger.info('Done processing %s. %s', srcdir,
                         self.timesince(atime))

        # Remove relevant FileHandler from logging
        self.logger.info('End logging to %s', dstlog)
        self.logconf.filehdlr = None

    @staticmethod
    def timesince(seconds):
        '''Utility function to format timedelta for logging'''
        return 'Total time ellapsed: %s' % timedelta(
            seconds=time.perf_counter() - seconds)


def main():
    '''entry point when run from commandline'''
    CruisePack()


if __name__ == "__main__":
    main()
