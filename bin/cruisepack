#!/usr/bin/env python3

# TODO: erase source dir when user requests it

'''cruisepack
    STS cruise data packer, in Python'''

import argparse
import bz2
import contextlib
import gzip
import hashlib
import logging
import logging.handlers
import lzma
import os
import tarfile
import tempfile
import time
import sys
from datetime import timedelta
from json import dumps
from multiprocessing import cpu_count
from multiprocessing import Pool
from multiprocessing.dummy import Pool as DummyPool
from pathlib import Path
from shutil import copyfileobj
from zipfile import ZipFile, ZIP_BZIP2, ZIP_LZMA


@contextlib.contextmanager
def pushd(new_dir):
    '''overloaded method for contextmanager to pushd/popd, Python-style
       using the "with" statement. For details, see
       stackoverflow.com/questions/6194499/pushd-through-os-system'''
    previous_dir = os.getcwd()
    os.chdir(new_dir)
    try:
        yield
    finally:
        os.chdir(previous_dir)


class CPError(Exception):
    '''Hack our own fatal error class'''
    def __init__(self, errmsg, exception=None):
        super().__init__()
        self.errmsg = errmsg
        self.exception = exception

    def __str__(self):
        if self.exception is not None:
            return '\n\t'.join((self.errmsg, '%s' % self.exception))
        return self.errmsg


class CruisePack:
    '''Set up a class to pack cruise data'''
    TIME_DURATION_UNITS = (('week', 604800), ('day', 86400),
                           ('hour', 3600), ('min', 60), ('second', 1))

    def __init__(self):
        atime = time.perf_counter()
        self.disclaimer = '(this can take a while...)'
        self.args = self.args_validate(self.args_parse())

        # Set up an empty list for self.args.compress_after == True
        self.compress_after = []

        # Set up console logging
        self.formatter = logging.Formatter(
            '%(asctime)s %(levelname)7s: %(message)s')
        self.logger = self.logger_config()

        self.logger.info('CLI arguments:\n%s',
                         dumps(self.args.__dict__, indent=1))

        for i, srcdir in enumerate(self.args.source_dir):
            self.pack_dir(srcdir, self.args.dstlog[i],
                          self.args.dstmd5[i], self.args.dstzip[i])

        # Multithread compression at the end, when requested
        if len(self.compress_after) > 0:
            self.logger.info('Compressing delayed tarballs')
            with DummyPool(processes=self.args.proc_count) as pool:
                pool.starmap(self.compress, self.compress_after)

        self.logger.info('Completed all processing. %s', self.timesince(atime))

    @staticmethod
    def args_parse():
        '''parse STDIN, if any'''

        parser = argparse.ArgumentParser(
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            description='''
                Build and execute a series of CLI rsync commands
                in order to synchronize data from our
                Data Acquisition Systems (DAS) to a central server.'''
        )

        parser.add_argument(
            '-s', '--source-dir',
            nargs='+',
            required=True,
            help='source directory(s) of files to archive. Multiple sources' +
            ' will create multiple destinations')
        parser.add_argument(
            '-d', '--destination-dir',
            default=None,
            nargs='+',
            help='destination directiory(s) to write arcive to. When None, ' +
            'the destination will be the parent dir of each source')
        parser.add_argument(
            '-C', '--compress-after',
            action='store_true',
            help='Compress tarball(s) after successfully writing to all ' +
            'destination(s). Given ample disk space, this can be useful ' +
            'for batch copying multiple cruise datasets to the archival ' +
            'space as quickly as possible, then running compression, which ' +
            'takes the longest amount of time. Futher, since compression is ' +
            'a singleton process, mutlithreading a number of jobs at the ' +
            'end can mean less overall wait time, when important')
        parser.add_argument(
            '-D', '--delete-after',
            action='store_true',
            help='Delete source(s) after successfully writing destination(s).')
        parser.add_argument(
            '-E', '--extension',
            type=str,
            default='tar.bz2',
            choices=('tar.bz2', 'tar.gz', 'tar.xz', 'tar', 'zip'),
            help='extension of target archive. NOTE: zip processing is ' +
            'in series whereas tar.bz2 is multithreaded')
        parser.add_argument(
            '-O', '--overwrite-destination',
            action='store_true',
            help='Overwrite destination(s), if destination(s) exist.')
        parser.add_argument(
            '-P', '--proc-count',
            default=cpu_count()*2-1,
            type=int,
            help='number of parallel md5sum jobs to run')

        return parser.parse_args()

    @staticmethod
    def args_validate(args):
        '''Validate CLI arguments'''
        if not isinstance(args.delete_after, bool):
            raise CPError

        if not isinstance(args.source_dir, list):
            raise CPError
        # Convert args to absolute path(s)
        for i, val in enumerate(args.source_dir):
            args.source_dir[i] = Path(val).absolute().__str__()

        if args.destination_dir is None:
            args.destination_dir = []
            for i in args.source_dir:
                args.destination_dir.append(
                    Path(i).parent.absolute().__str__())

        if not isinstance(args.destination_dir, list):
            raise CPError

        # Calculate destinations
        dst = []

        if len(args.destination_dir) == 1:
            for i in args.source_dir:
                dst.append(
                    Path.joinpath(Path(args.destination_dir[0]),
                                  '%s' % Path(i).name).__str__())
        elif len(args.source_dir) == len(args.destination_dir):
            for i, val in enumerate(args.destination_dir):
                dst.append(
                    Path.joinpath(Path(val),
                                  '%s' % Path(
                                      args.source_dir[i]).name).__str__())
        else:
            raise CPError(
                "Don't know how to process %d sources and %d destinations"
                % (len(args.source_dir), len(args.destination_dir)))

        args.dstlog = ['%s.log.txt' % i for i in dst]
        args.dstmd5 = ['%s.md5.txt' % i for i in dst]
        args.dstzip = ['%s.%s' % (i, args.extension) for i in dst]

        return args

    def chkdst(self, dstfiles):
        '''Check to see if destination files exist and decide what to
           about it'''
        process = False
        if isinstance(dstfiles, str):  # ensure list
            dstfiles = [dstfiles]
        if not isinstance(dstfiles, list):  # ensure list
            raise CPError('chkdst requires a str on list of str')
        for i in dstfiles:
            if not os.path.exists(i):  # good to go
                process = True
            elif self.args.overwrite_destination is True:  # also good
                # Avoid overwriting file(s) if we did not call for it
                self.logger.warning(
                    '%s exists. overwrite_destination enabled. Overwriting!',
                    i)
                process = True
            else:
                # Avoid overwriting file(s) if we did not call for it
                self.logger.error(
                    '%s exists and overwrite_destination not set.',
                    i)
                process = False
        if process is False:
            self.logger.warning('Exiting due to exisiting files.')
            sys.exit(1)

    def compress(self, src, dst, compress):
        '''Compress a file'''
        # Validation
        self.chkdst(dst)
        if compress not in ('bz2', 'gz', 'xz'):
            raise CPError('Unsupported compression type, %s' % compress)
        srcsize = os.path.getsize(src)

        # Create compressed file
        atime = time.perf_counter()
        self.logger.info('Compress %s to %s %s', src, dst, self.disclaimer)

        try:
            with open(src, 'rb') as fpsrc:
                if compress == 'bz2':
                    with bz2.open(dst, 'wb') as fpdst:
                        copyfileobj(fpsrc, fpdst)
                elif compress == 'gz':
                    with gzip.open(dst, 'wb') as fpdst:
                        copyfileobj(fpsrc, fpdst)
                elif compress == 'xz':
                    with lzma.open(dst, 'wb') as fpdst:
                        copyfileobj(fpsrc, fpdst)
        except KeyboardInterrupt:
            self.logger.warning(
                'Received keyboard interrupt. Deleting %s and quitting',
                dst)
            os.unlink(dst)
            sys.exit(3)

        self.logger.info('Done creating %s. %s', dst, self.timesince(atime))
        dstsize = os.path.getsize(dst)
        self.logger.info(
            'Disk usage of %s is %s (%0.2f%% smaller than %s)', dst,
            self.humanbytes(dstsize), 100-100*(dstsize/srcsize), src)
        self.logger.debug('Attempt to delete %s.', src)
        os.unlink(src)
        self.logger.debug('Deleted %s.', src)

    def getfilelist(self, path):
        '''Get a list of all files under a directory'''
        filelist = []
        self.logger.debug('Obtain list of files under %s', path)
        atime = time.perf_counter()

        for root, _, files in os.walk(path):
            for myfile in files:
                fullpath = os.path.join(root, myfile)
                filelist.append(fullpath)
        self.logger.info('Found %d files in %s. %s',
                         len(filelist), path, self.timesince(atime))
        return filelist

    def getfilesize(self, path):
        '''wrapper method of os.path.getsize() to deal with errant
           symlinks; they cause an exception, but we do not care.
           We'll just log that it happened and move on'''
        try:
            return os.path.getsize(path)
        except FileNotFoundError as err:
            # Sometimes datasets have symlinks from other filesystems
            # that will not resolve to the original file they're linked
            # to.  While not great, we don't care that hard, so we'll
            # just note that it happened and move on
            if os.path.islink(path):
                self.logger.warning(
                    '%s generated an error, but is a symlink. Ignoring.',
                    path)
                return 0

            # Some condition we did not expect? Raise an exception
            raise CPError(err)

    def getmeta_filesize(self, path, filelist):
        '''file size metadata operations'''
        atime = time.perf_counter()
        self.logger.info(
            'Calculate total disk size used by %d files in %s %s',
            len(filelist), path, self.disclaimer)

        # Calculate sum of file sizes for every file on our list
        # Multithreaded, despite multiprocess being better for
        # I/O-bound tasks. This is because this task is not very
        # slow, and multithread makes it easy for logging of warnings
        with DummyPool(processes=self.args.proc_count) as pool:
            bytes_used = sum(pool.map(self.getfilesize, filelist))

        self.logger.info('Total disk usage of %s: %s. %s',
                         path, self.humanbytes(bytes_used),
                         self.timesince(atime))
        return bytes_used

    def getmeta_md5sum(self, path, filelist, md5path):
        '''md5 metadata operations'''
        atime = time.perf_counter()
        self.logger.info('Calculate md5sum on %d files in %s %s',
                         len(filelist), path, self.disclaimer)

        # Calculate md5sum for every file on our list. Multiprocess for
        # best performance for these CPU-bound and I/O-bound tasks.
        with Pool(processes=self.args.proc_count) as pool:
            md5s = pool.map(self.md5sum, filelist)
        md5s.sort()

        # Save sorted list to a plaintext file
        self.logger.debug('Attempt to save md5sum results saved to %s',
                          md5path)
        with open(md5path, 'w') as filep:
            filep.write('\n'.join(
                ['{}\t{}'.format(i[1], i[0]) for i in md5s]))
        self.logger.info('md5sum results saved to %s. %s',
                         md5path, self.timesince(atime))

    @staticmethod
    def humanbytes(num):
        '''Convert a number (bytes) to human-readable bytes'''
        symbols = ('K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')
        prefix = {}
        for i, sym in enumerate(symbols):
            prefix[sym] = 1 << (i+1)*10
        for sym in reversed(symbols):
            if num >= prefix[sym]:
                return '%.1f%sB' % (num/prefix[sym], sym)
        return '%sB' % num

    def logger_config(self):
        '''Set up class logger'''
        self.logconf = argparse.Namespace(
            consolehdlr=logging.StreamHandler(), filehdlr=None)
        logger = logging.getLogger('CruisePack')

        # pass all messages down to handlers
        logger.setLevel(logging.DEBUG)

        # console logger
        self.logconf.consolehdlr.setLevel(logging.INFO)
        self.logconf.consolehdlr.setFormatter(self.formatter)
        logger.addHandler(self.logconf.consolehdlr)

        return logger

    def logger_filehandler_add(self, dstlog):
        '''Manage logging.FileHandler. We only ever have one open at
           a time'''

        try:
            self.logconf.filehdlr = logging.FileHandler(dstlog, mode='w')
        except (FileNotFoundError, PermissionError) as err:
            raise CPError('Not able to open log file %s! %s' % (dstlog, err))

        # Add new FileHandler to logger
        self.logconf.filehdlr.setFormatter(self.formatter)
        self.logconf.filehdlr.setLevel(logging.DEBUG)
        self.logger.addHandler(self.logconf.filehdlr)

    @staticmethod
    def md5sum(path):
        '''Open a file, calculate md5sum'''
        hash_md5 = hashlib.md5()
        try:
            with open(path, 'rb') as filep:
                for chunk in iter(lambda: filep.read(4096), b''):
                    hash_md5.update(chunk)
        except FileNotFoundError as err:
            # If we got a symlink that points to some non-local
            # file, we do not care that hard; make a dummy hash
            # and move on
            if os.path.islink(path):
                return (path, '00000000000000000000000000000000')
            # Fail otherwise
            raise CPError(err)
        return (path, hash_md5.hexdigest())

    def mkarchive(self, filename, files, md5file):
        '''Make a supported archive file'''
        atime = time.perf_counter()
        self.logger.info('Create %s %s', filename, self.disclaimer)
        self.logger.debug('"%s" mode selected', self.args.extension)

        # Rename compressed tarball filename to just .tar
        compression = False
        if filename.endswith(('.tar.bz2', '.tar.gz', '.tar.xz')):
            strings = filename.split('.')
            filename_orig = filename
            filename = '.'.join(strings[:-1])
            compression = strings[-1]
            self.logger.info(
                'Compression "%s" will be processed after tarball creation.',
                compression)

        # Calculate a unique filename in our destination dir
        _, tmpf = tempfile.mkstemp(dir=Path(filename).parent,
                                   prefix='%s.' % Path(filename).name)

        self.logger.info('Write %s (will later move to %s).', tmpf, filename)

        try:
            if self.args.extension == 'zip':
                self.mkzip(tmpf, files, md5file)
            else:
                # Compression is based on extension selected
                self.mktar(tmpf, files, md5file)
        except KeyboardInterrupt:
            self.logger.warning(
                'Received keyboard interrupt. Deleting %s and quitting',
                tmpf)
            os.unlink(tmpf)
            sys.exit(2)

        self.logger.info('Wrote %s.', tmpf)

        self.move(tmpf, filename)

        self.logger.info('%s size is %s; mdsum is %s',
                         filename,
                         self.humanbytes(os.path.getsize(filename)),
                         self.md5sum(filename)[1])

        # Conditionally compress file (now or later) and return filename
        # of where we are leaving things.
        if compression:
            if self.args.compress_after:
                self.compress_after.append((filename, filename_orig,
                                            compression))
                self.logger.warning('Creating %s->%s has been delayed',
                                    filename, filename_orig)
                return filename

            self.compress(filename, filename_orig, compression)
            return filename_orig

        self.logger.info('Created %s. %s', filename, self.timesince(atime))
        return filename

    @staticmethod
    def mktar(filename, files, md5file):
        '''Make a tarball based provided filename and list of files'''

        # Only write an uncompressed .tar file; we want to copy raw
        # bytes to our archive location as quickly as possible, and
        # then save disk space, afterwards.
        with tarfile.open(filename, 'w:') as archive:

            # First, archive our md5sum file and then delete it
            md5path = Path(md5file)
            with pushd(md5path.parent):
                archive.add(md5path.name)
            os.unlink(md5file)

            # Multithreading or multiprocessing would be nice, here.
            # However, even when seemingly supported, neither seems
            # truly ready for use.  In testing, even when no expections
            # occurred, resulting tarballs were corrupt. Regardless of
            # data integrity, performance improvements were also not
            # dramatically better. For now, we just add files to the
            # archive, in series, in order to play it safest.
            for myf in files:
                archive.add(myf)

    @staticmethod
    def mkzip(filename, files, md5file):
        '''Make a zip file based provided filename and list of files'''
        try:
            # Failover to LZMA if needed
            archive = ZipFile(filename, 'w', compression=ZIP_BZIP2)
        except NotImplementedError:
            # If we don't have LZMA nor BZIP2 capability, fail
            archive = ZipFile(filename, 'w', compression=ZIP_LZMA)
        else:
            # First, archive our md5sum file and then delete it
            md5path = Path(md5file)
            with pushd(md5path.parent):
                archive.write(md5path.name)
            os.unlink(md5file)

            # See multithreading note in self.mktar(). Just add files
            # in series, for now
            for myf in files:
                archive.write(myf)
            archive.close()

    def move(self, src, dst):
        '''Move a file'''
        self.chkdst(dst)
        try:
            self.logger.info('Move %s to %s.', src, dst)
            os.rename(src, dst)
        except FileExistsError:
            if self.args.overwrite_destination:
                self.logger.warning('%s exists. Removing.', dst)
                os.remove(dst)
                os.rename(src, dst)
                return
            self.logger.error(
                '%s already exists. Quit and clean up %s.',
                dst, src)
            os.remove(src)
            sys.exit(3)
        except PermissionError as err:
            raise CPError('Cannot move %s to %s. %s' % (src, dst, err))

    def pack_dir(self, srcdir, dstlog, dstmd5, dstzip):
        '''Dig through a dir, md5sum every file and archive the dir'''
        atime = time.perf_counter()

        self.chkdst([dstzip, dstlog, dstmd5])

        # Add relevant FileHandler to logging
        self.logger_filehandler_add(dstlog)
        self.logger.info('Processing directory %s', srcdir)
        self.logger.info('Start logging to %s', dstlog)

        # Tarballs and zipfiles should not have absolute paths in them,
        # so we want to process our data in the parent of the directory
        # in question.
        srcpath = Path(srcdir)
        with pushd(srcpath.parent.__str__()):
            self.logger.info('Changed directory to %s',
                             srcpath.parent.__str__())
            filelist = self.getfilelist(srcpath.name)
            self.getmeta_filesize(srcpath.name, filelist)
            self.getmeta_md5sum(srcpath.name, filelist, dstmd5)
            self.mkarchive(dstzip, filelist, dstmd5)

        self.logger.info('Done processing %s. %s', srcdir,
                         self.timesince(atime))

        # Remove relevant FileHandler from logging
        self.logger.info('End logging to %s', dstlog)
        self.logconf.filehdlr = None

    @staticmethod
    def timesince(seconds):
        '''Utility function to format timedelta for logging'''
        return 'Total time ellapsed: %s' % timedelta(
            seconds=time.perf_counter() - seconds)


def main():
    '''entry point when run from commandline'''
    CruisePack()


if __name__ == "__main__":
    main()
